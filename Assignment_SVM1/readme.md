# Generate a separable dataset of size 1000 and 2 features. Plot the samples on a graph and mark the support vectors for the dataset.
1.python file https://github.com/vedamurthy04/assignment/blob/master/Assignment_SVM1/svm_1.py


2.juypter file https://github.com/vedamurthy04/assignment/blob/master/Assignment_SVM1/svm_1a.ipynb
# show that changing the vectors other than the support vectors has no effect on the decision boundary

In the second graph plot a linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes.

As in the SVM or first plot the value is random for the sepration of vectors. No matter what values we give the decission boundries goes through only supportive vectors. Thus that proves changing the vectors other than the support vectors has no effect on the decision boundary.
for more info 
click here https://github.com/vedamurthy04/assignment/blob/master/Assignment_SVM1/support%20vector.docx
